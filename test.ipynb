{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40c59d4",
   "metadata": {},
   "source": [
    "# 1.简单改造，Conll2003数据集作测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c07ce4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm,trange\n",
    "import os\n",
    "class GetData:\n",
    "    def read(self, data_path):\n",
    "        data_parts = ['train', 'valid', 'test']\n",
    "        extension = '.txt'\n",
    "        dataset = {}\n",
    "        bar = tqdm(data_parts)\n",
    "        for data_part in bar:\n",
    "            bar.set_description(\"正在读取数据集\")\n",
    "            file_path = os.path.join(data_path, data_part+extension)\n",
    "            dataset[data_part] = self.read_file(str(file_path))\n",
    "            if data_part == 'test':\n",
    "                bar.set_description('数据读取完毕')\n",
    "        return dataset\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        samples = []\n",
    "        tokens = []   # 单词\n",
    "        tags = []     # 实体标注\n",
    "        with open(file_path,'r', encoding='utf-8') as fb:\n",
    "            for line in fb:\n",
    "                line = line.strip('\\n')\n",
    "                if line == '-DOCSTART- -X- -X- O':   # 去除数据头\n",
    "                    pass\n",
    "                elif line =='':                      # 一句话结束\n",
    "                    if len(tokens) != 0:\n",
    "                        samples.append((tokens, tags))\n",
    "                        tokens = []\n",
    "                        tags = []\n",
    "                else:\n",
    "                    items = line.split(' ')\n",
    "                    tokens.append(items[0])\n",
    "                    tags.append(items[-1])\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0578d28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18aac5d3fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)    # 人工设定随机种子以保证相同的初始化参数，实现模型的可复现性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b52b7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):  # 给定输入二维序列，取每行（第一维度）的最大值，返回对应索引。\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):    # 利用to_ix这个word2id字典，将序列seq中的词转化为数字表示，包装为torch.long后返回\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):                # 函数目的相当于log∑exi 首先取序列中最大值，输入序列是一个二维序列(shape[1,tags_size])。下面的计算先将每个值减去最大值，再取log_sum_exp，最后加上最大值。\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06045051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # 词嵌入维度，即输入维度\n",
    "        self.hidden_dim = hidden_dim   # 隐层维度\n",
    "        self.vocab_size = vocab_size   # 训练集词典大小\n",
    "        self.tag_to_ix = tag_to_ix     # 标签索引表\n",
    "        self.tagset_size = len(tag_to_ix) # 标注 类型数\n",
    "        print(f'tagset_size={self.tagset_size}')\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)  # （词嵌入的个数，嵌入维度）\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,         # （输入节点数，隐层节点数，隐层层数，是否双向）\n",
    "                            num_layers=1, bidirectional=True)       #  hidden_size除以2是为了使BiLSTM的输出维度依然是hidden_size,而不用乘以2\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)   # （输入x的维度，输出y的维度），将LSTM的输出线性映射到标签空间\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(                            # 转移矩阵，标注j转移到标注i的概率，后期要学习更新\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000     # 不会有标注转移到开始标注\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000      # 结束标注不会转移到其他标注\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):                                          # 初始化隐层（两层，3维）\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),            # (num_layer * num_direction, batch_size)\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))            # (隐层层数2 * 方向数1， 批大小1， 每层节点数)\n",
    "\n",
    "    def _forward_alg(self, feats):                                  # 得到所有路径的分数/概率\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)    # P，(1, m)维，初始化为-10000\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas                                   # 前向状态，记录当前t之前的所有路径的分数\n",
    "  \n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:                                          # 动态规划思想，具体见onenote上的笔记\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha                                                # 返回的是所有路径的分数\n",
    "\n",
    "    def _get_lstm_features(self, sentence):             # 通过BiLSTM层，输出得到发射分数\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)      # 对输入语句 词嵌入化\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)              # 词嵌入通过lstm网络输出,lstm传入参数之后会自动调用其forward方法\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)            # 将输出转为2维（原本是3维，但是batch_size=1，可以去掉这一维）\n",
    "        lstm_feats = self.hidden2tag(lstm_out)                              # 将输出映射到标签空间，得到单词-分数表\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):             # 计算给定路径的分数\n",
    "        # feats : LSTM的所有输出，发射分数矩阵\n",
    "        # tags : golden路径的标注序列\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])   # 在标注序列最前加上开始标注\n",
    "        for i, feat in enumerate(feats):                                                        # 计算给定序列的分数，Σ发散分数+Σ转移分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)                      # 初始化forward_var,并且 开始标注 的分数为0,确保一定是从START_TAG开始的,\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars                                                     # forward_var记录每个标签的前向状态得分，即w{i-1}被打作每个标签的对应得分值\n",
    "        for feat in feats:                                                           # feats是LSTM的输出，每一个feat都是一个词w{i}，feat[tag]就是这个词tag标注的分数\n",
    "            bptrs_t = []  # holds the backpointers for this step                     # 记录当前词w{i}对应每个标签的最优转移结点\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step          # 记录当前词各个标签w{i, j}对应的最高得分\n",
    "                                                                                     # 动态规划：w{i，j}=max{forwar_var + transitions[j]}，词存于bptrs_t中，分数存于viterbivars_t中\n",
    "\n",
    "            for next_tag in range(self.tagset_size):                                 # 对当前词w{i}的每个标签 运算\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)                                              # 记忆，方便回溯\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)                                            # 结束标记前的一个词的最高前向状态得分就是最优序列尾\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):                                        # 回溯\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):       # CRF的损失函数：-gold分数-logsumexp(所有序列)\n",
    "        feats = self._get_lstm_features(sentence)                   # 通过BiLSTM层，获得每个 {词-标签}对 的发射分数\n",
    "        forward_score = self._forward_alg(feats)                    # 根据发射分数计算所有路径的分数\n",
    "        gold_score = self._score_sentence(feats, tags)              # 传入标注序列真实值，计算语句的真实分数gold_score\n",
    "        return forward_score - gold_score                           # 返回误差值\n",
    "\n",
    "    def forward(self, sentence):                                    # 重载前向传播函数，对象传入参数后就会自动调用该函数\n",
    "        # Get the emission scores from the BiLSTM \n",
    "        lstm_feats = self._get_lstm_features(sentence)              # 通过LSTM层得到输出\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)           # 通过CFR层得到最优路径及其分数\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7a1a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4e6263becb42d48d1106c5251111e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5       # 词嵌入维度\n",
    "HIDDEN_DIM = 4          # 隐层层数\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "ds_rd = GetData()\n",
    "data = ds_rd.read(\"./data\")\n",
    "training_data = data['train'][0:10]\n",
    "\n",
    "word_to_ix = {}                             # 训练集词典 {词——索引}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B-PER\": 0, \"B-LOC\": 1, \"B-ORG\": 2, \"B-MISC\": 3,\n",
    "             \"I-PER\": 4, \"I-LOC\": 5, \"I-ORG\": 6, \"I-MISC\": 7,\n",
    "             \"O\": 8, START_TAG: 9, STOP_TAG: 10}  # 标签词典 {标注——索引}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ea13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)   # 模型BiLSTM-CRF\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)       # 优化器：使用SGD更新参数\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():                                                       # 在训练前测试一次预测结果，和训练后对比\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)       # 将第一个训练样本（词序列）转成索引序列\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long) # 将第一个训练样本的标签转成索引序列\n",
    "    print(model(precheck_sent))                                             # 输出第一次预测的结果（model(·)自动调用forward函数）\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in trange(300,desc='模型训练进度'):  # again, normally you would NOT do 300 epochs, it is toy data     # 训练，迭代300次\n",
    "    bar = tqdm(training_data, leave=False)\n",
    "    for sentence, tags in bar:\n",
    "        bar.set_description(f'epoch【{epoch}】')\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()                                                                   # 每次迭代前梯度清零（因为默认会叠加梯度）\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)                                # 输入：语句转为词索引\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)              # 真实值：标注序列转为索引\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)                               # 计算误差\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()                                                                     # 计算当前梯度，反向传播\n",
    "        optimizer.step()                                                                    # 根据当前梯度更新网络参数\n",
    "\n",
    "# Check predictions after training\n",
    "# with torch.no_grad():                                                      # 在训练后预测一次预测，和训练前对比\n",
    "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     print(model(precheck_sent))\n",
    "print('traning over!')\n",
    "torch.save(model,'pre_model.pth')                                  # 保存模型\n",
    "torch.save(model.state_dict(),'model_params.pth')                  # 保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f92b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型预测输出: (tensor(72.7139), [1, 8, 3, 8, 8, 8, 3, 8, 8])\n",
      "真值: tensor([2, 8, 3, 8, 8, 8, 3, 8, 8])\n",
      "预测中词性标注错误的个数: -1\n",
      "tensor(72.7139)\n",
      "[1, 8, 3, 8, 8, 8, 3, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(9.0805), [0, 4])\n",
      "真值: tensor([0, 4])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(9.0805)\n",
      "[0, 4]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(14.2910), [1, 8])\n",
      "真值: tensor([1, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(14.2910)\n",
      "[1, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(267.7945), [8, 2, 6, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 2, 6, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(267.7945)\n",
      "[8, 2, 6, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(266.3945), [1, 8, 8, 8, 8, 2, 6, 8, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([1, 8, 8, 8, 8, 2, 6, 8, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1,\n",
      "        8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(266.3945)\n",
      "[1, 8, 8, 8, 8, 2, 6, 8, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(291.1624), [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 0, 4, 4, 4, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8,\n",
      "        0, 4, 4, 4, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(291.1624)\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 0, 4, 4, 4, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(230.4059), [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6,\n",
      "        8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(230.4059)\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(380.2174), [8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(380.2174)\n",
      "[8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(217.0059), [0, 8, 3, 8, 8, 8, 8, 1, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 7, 7, 8, 3, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([0, 8, 3, 8, 8, 8, 8, 1, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 7, 7, 8, 3, 8, 8,\n",
      "        8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(217.0059)\n",
      "[0, 8, 3, 8, 8, 8, 8, 1, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 7, 7, 8, 3, 8, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n",
      "模型预测输出: (tensor(348.3690), [8, 0, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 0, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "tensor(348.3690)\n",
      "[8, 0, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "<class 'int'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_model = torch.load('pre_model.pth')                            # 直接加载模型\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        test_data = data['train'][i]\n",
    "        In = prepare_sequence(test_data[0],word_to_ix)\n",
    "        Out= pre_model(In)\n",
    "        print(f'模型预测输出: {Out}')\n",
    "        targets = torch.tensor([tag_to_ix[tag] for tag in test_data[1]], dtype=torch.long)\n",
    "        print(f'真值: {targets}')\n",
    "        print(f'预测中词性标注错误的个数: {(torch.tensor(Out[1],dtype=torch.long)-targets).sum().item()}')\n",
    "        print(Out[0])\n",
    "        print(Out[1])\n",
    "        print(type(Out[1][0]))\n",
    "        print()\n",
    "        # We got it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5109fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagset_size=11\n",
      "模型预测输出: (tensor(70.8756), [1, 8, 3, 8, 8, 8, 3, 8, 8])\n",
      "真值: tensor([2, 8, 3, 8, 8, 8, 3, 8, 8])\n",
      "预测中词性标注错误的个数: -1\n",
      "\n",
      "模型预测输出: (tensor(10.9569), [0, 4])\n",
      "真值: tensor([0, 4])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(14.2688), [1, 8])\n",
      "真值: tensor([1, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(267.0723), [8, 2, 6, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 2, 6, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(263.2603), [1, 8, 8, 8, 8, 2, 6, 8, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([1, 8, 8, 8, 8, 2, 6, 8, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1,\n",
      "        8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(292.0675), [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 0, 4, 4, 4, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8,\n",
      "        0, 4, 4, 4, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(232.4763), [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 6,\n",
      "        8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(381.3405), [8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 0, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(216.3839), [0, 8, 3, 8, 8, 8, 8, 1, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 7, 7, 8, 3, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([0, 8, 3, 8, 8, 8, 8, 1, 8, 1, 8, 8, 8, 8, 8, 8, 8, 3, 7, 7, 8, 3, 8, 8,\n",
      "        8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n",
      "模型预测输出: (tensor(348.6090), [8, 0, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "真值: tensor([8, 0, 8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "预测中词性标注错误的个数: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)           # 读取参数加载模型\n",
    "model2.load_state_dict(torch.load('model_params.pth'))\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        test_data = data['train'][i]\n",
    "        In = prepare_sequence(test_data[0],word_to_ix)\n",
    "        Out = model2(In)\n",
    "        print(f'模型预测输出: {Out}')\n",
    "        targets = torch.tensor([tag_to_ix[tag] for tag in test_data[1]], dtype=torch.long)\n",
    "        print(f'真值: {targets}')\n",
    "        print(f'预测中词性标注错误的个数: {(torch.tensor(Out[1],dtype=torch.long)-targets).sum().item()}')\n",
    "        print()\n",
    "        # We got it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04d25333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0999c79d89d84c9fa0a55fbeda5f6eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "训练中:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2nd loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tqdm.notebook import trange, tqdm\n",
    "# from time import sleep\n",
    "# out_bar = trange(5, desc='训练中',colour='red')\n",
    "# for i in out_bar:\n",
    "#     bar = tqdm(range(2), desc='2nd loop', leave=False)\n",
    "#     for j in bar:\n",
    "#         bar.set_description(f'epoch【{i}】')\n",
    "#         sleep(0.5)\n",
    "#     if i == 4:\n",
    "#         out_bar.set_description('训练完毕')\n",
    "        \n",
    "# #     out_bar.set_description('总进度: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592137a6",
   "metadata": {},
   "source": [
    "# 2.输入（Json文件）处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11547f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pandas-1.4.3-cp38-cp38-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.4.3\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34ba32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e017aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[['NAME', [3, 14]], ['TICKER', [35, 39]], ['NOTIONAL', [21, 34]]]\n",
      "2400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear 568.763million AXNVF</td>\n",
       "      <td>[[TICKER, [20, 25]], [NOTIONAL, [5, 19]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy 703.363thousand HEOFF</td>\n",
       "      <td>[[TICKER, [20, 25]], [NOTIONAL, [4, 19]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May I 927.795hundred RLXXF Put</td>\n",
       "      <td>[[TICKER, [21, 26]], [NOTIONAL, [6, 20]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.574hundred BRGGF</td>\n",
       "      <td>[[TICKER, [14, 19]], [NOTIONAL, [0, 13]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Mark Romero Can I 66.585million ABST Thank ...</td>\n",
       "      <td>[[NAME, [3, 14]], [TICKER, [35, 39]], [NOTIONA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>860.036 PCLOF Buy</td>\n",
       "      <td>[[TICKER, [8, 13]], [NOTIONAL, [0, 7]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Jennifer Long Can I 956.436thousand CAE call ...</td>\n",
       "      <td>[[NAME, [1, 14]], [TICKER, [37, 40]], [NOTIONA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>May I 282.922billion FTRP</td>\n",
       "      <td>[[TICKER, [21, 25]], [NOTIONAL, [6, 20]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>49.527trillion NSCIF sell</td>\n",
       "      <td>[[TICKER, [15, 20]], [NOTIONAL, [0, 14]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Hi Megan Morrison , 961.914thousand EXN buy</td>\n",
       "      <td>[[NAME, [3, 17]], [TICKER, [36, 39]], [NOTIONA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0                             Dear 568.763million AXNVF   \n",
       "1                             Buy 703.363thousand HEOFF   \n",
       "2                        May I 927.795hundred RLXXF Put   \n",
       "3                                   77.574hundred BRGGF   \n",
       "4     Hi Mark Romero Can I 66.585million ABST Thank ...   \n",
       "...                                                 ...   \n",
       "2995                                  860.036 PCLOF Buy   \n",
       "2996   Jennifer Long Can I 956.436thousand CAE call ...   \n",
       "2997                          May I 282.922billion FTRP   \n",
       "2998                          49.527trillion NSCIF sell   \n",
       "2999        Hi Megan Morrison , 961.914thousand EXN buy   \n",
       "\n",
       "                                                  label  \n",
       "0             [[TICKER, [20, 25]], [NOTIONAL, [5, 19]]]  \n",
       "1             [[TICKER, [20, 25]], [NOTIONAL, [4, 19]]]  \n",
       "2             [[TICKER, [21, 26]], [NOTIONAL, [6, 20]]]  \n",
       "3             [[TICKER, [14, 19]], [NOTIONAL, [0, 13]]]  \n",
       "4     [[NAME, [3, 14]], [TICKER, [35, 39]], [NOTIONA...  \n",
       "...                                                 ...  \n",
       "2995            [[TICKER, [8, 13]], [NOTIONAL, [0, 7]]]  \n",
       "2996  [[NAME, [1, 14]], [TICKER, [37, 40]], [NOTIONA...  \n",
       "2997          [[TICKER, [21, 25]], [NOTIONAL, [6, 20]]]  \n",
       "2998          [[TICKER, [15, 20]], [NOTIONAL, [0, 14]]]  \n",
       "2999  [[NAME, [3, 17]], [TICKER, [36, 39]], [NOTIONA...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = './data/data.json'\n",
    "df = pd.read_json(file)\n",
    "a, b = df.loc[4]\n",
    "print(type(a))\n",
    "print(b)\n",
    "print(len(df)//5*4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5aa62e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9, 16, 18, 22, 24, 28, 44, 50]\n",
      "[['Name', [3, 16]], ['NOTIONAL', [29, 44]], ['TICKER', [45, 50]]]\n",
      "[(3, 'Nam'), (16, 'Nam'), (29, 'NOT'), (44, 'NOT'), (45, 'TIC'), (50, 'TIC')]\n",
      "3  2 16\n",
      "3  9 16\n",
      "3  16 16\n",
      "3  18 16\n",
      "29  18 44\n",
      "29  22 44\n",
      "29  24 44\n",
      "29  28 44\n",
      "29  44 44\n",
      "29  50 44\n",
      "45  50 50\n",
      "['O', 'B-Nam', 'I-Nam', 'O', 'O', 'O', 'O', 'B-NOT', 'B-TIC']\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix = {}                             # 训练集词典 {词——索引}\n",
    "# for sentence, tags in training_data:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "# START_TAG = \"<START>\"\n",
    "# STOP_TAG = \"<STOP>\"\n",
    "# tag_to_ix = {\"B-NAM\": 0, \"B-TIC\": 1, \"B-NOT\": 2, \n",
    "#              \"I-NAM\": 3, \"I-TIC\": 4, \"I-NOT\": 5,\n",
    "#              \"O\": 6, START_TAG: 7, STOP_TAG: 8}  # 标签词典 {标注——索引}\n",
    "\n",
    "# word_to_ix = {}                             # 训练集词典 {词——索引}\n",
    "# for sentence, tags in training_data:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "# START_TAG = \"<START>\"\n",
    "# STOP_TAG = \"<STOP>\"\n",
    "# tag_to_ix = {\"B-NAM\": 0, \"B-TIC\": 1, \"B-NOT\": 2, \n",
    "#              \"I-NAM\": 3, \"I-TIC\": 4, \"I-NOT\": 5,\n",
    "#              \"O\": 6, START_TAG: 7, STOP_TAG: 8}  # 标签词典 {标注——索引}\n",
    "\n",
    "def get_tags(label, tokens):\n",
    "    length = len(tokens)\n",
    "    tags = ['O'] * length\n",
    "    index = []*length\n",
    "    last = 0\n",
    "    for i in range(length):\n",
    "        index.append(last+len(tokens[i]))\n",
    "        last = index[-1]+1\n",
    "#     print(index)\n",
    "    label.sort(key = lambda x: x[1][0])\n",
    "    help_map = {0:'B-', 1:'I-'}\n",
    "#     print(label)\n",
    "    label2 = [(l, x[0:3]) for x, y in label for l in y]\n",
    "#     print(label2)\n",
    "    i, j = 0, 0\n",
    "    while i < length and j < len(label):    # 双指针\n",
    "#         print(f'{label[j][1][0]}  {index[i]} {label[j][1][1]}')\n",
    "        if label[j][1][0] < index[i] <= label[j][1][1]:\n",
    "            tags[i] = label[j][0][0:3]\n",
    "        if index[i] > label[j][1][1]:\n",
    "            j+=1\n",
    "            i-=1\n",
    "        i += 1\n",
    "    for i in range(length-1, -1, -1):\n",
    "        if tags[i] != 'O':\n",
    "            if i == 0 or tags[i-1] != tags[i]:\n",
    "                tags[i] = f'B-{tags[i]}'\n",
    "            else:\n",
    "                tags[i] = f'I-{tags[i]}'\n",
    "    return tags\n",
    "# test_label = [['TICKER', [21, 26]], ['NOTIONAL', [6, 20]]]\n",
    "# sentence = 'May I 927.795hundred RLXXF Put'\n",
    "\n",
    "# # test_label = [['Name', [3, 16]],['TICKER', [37, 41]], ['NOTIONAL', [23, 36]]]\n",
    "# # sentence = 'Hi Mark Romero K Can I 66.585million ABST Thank you.'\n",
    "\n",
    "# test_label = [['Name', [3, 16]],['TICKER', [45, 50]], ['NOTIONAL', [29, 44]]]\n",
    "# sentence = 'Hi Daniel Joseph , May I Buy 678.559thousand PSHIF '\n",
    "\n",
    "# print(get_tags(test_label, sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31a4e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取训练数据: 100%|█████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 17062.47it/s]\n",
      "读取测试数据: 100%|███████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 17694.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n",
      "4270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_size = len(df)\n",
    "data_set = {'training': [], 'test': []}\n",
    "tokens = []\n",
    "tags = []\n",
    "word_to_ix = {}\n",
    "for i in tqdm(range(0, data_size//5*4), desc='读取训练数据', position=0):\n",
    "    text, label = df.loc[i]\n",
    "    tokens = text.split()\n",
    "    tags = get_tags(label, len(text))\n",
    "    for word in tokens:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    data_set['training'].append((tokens, tags))\n",
    "for i in tqdm(range(data_size//5*4, data_size), desc='读取测试数据', position=0):\n",
    "    text, label = df.loc[i]\n",
    "    tokens = text.split()\n",
    "    tags = get_tags(label, len(text))\n",
    "    for word in tokens:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    data_set['test'].append((tokens, tags))\n",
    "print(len(data_set['training']))\n",
    "print(len(data_set['test']))\n",
    "print(len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c38a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c612333",
   "metadata": {},
   "source": [
    "# 3.损失函数测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50929ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "SmoothL1Loss tensor(0.1067)\n",
      "SmoothL1Loss2 tensor(0.0817)\n",
      "SmoothL1Loss2 tensor(0.1083)\n",
      "tensor([[ 0.6123,  1.4574, -0.8892,  ...,  0.0050,  0.4898,  0.2029],\n",
      "        [-0.3059,  0.4671,  0.7391,  ..., -0.3981,  1.7851, -1.0114],\n",
      "        [-0.2853,  0.3888,  1.3631,  ..., -0.6380,  1.0693, -0.2788],\n",
      "        ...,\n",
      "        [-0.8388, -2.1040, -0.5288,  ..., -1.7141,  1.0215,  0.1173],\n",
      "        [-0.0771,  0.6776,  2.5670,  ..., -0.4080,  1.2608, -0.1725],\n",
      "        [ 0.7068, -1.2096, -0.9916,  ..., -1.1053,  0.1050, -0.1743]])\n",
      "tensor([[-4.5344e-01, -1.1273e+00,  9.2994e-01,  ...,  3.9153e-01,\n",
      "         -1.0993e+00, -8.1346e-01],\n",
      "        [ 8.1977e-01, -6.0418e-01,  1.2448e+00,  ..., -7.7001e-04,\n",
      "          5.8065e-01, -3.2354e-01],\n",
      "        [-6.2533e-01,  7.8292e-02,  3.0498e-01,  ...,  3.3939e-01,\n",
      "         -3.9383e-01,  2.5305e-01],\n",
      "        ...,\n",
      "        [-6.8670e-01,  9.1317e-01, -1.1309e+00,  ...,  1.4562e+00,\n",
      "          1.5861e-02,  6.3571e-01],\n",
      "        [ 9.9312e-01, -6.1834e-01,  1.3358e-01,  ...,  2.0359e+00,\n",
      "          8.5639e-01, -1.4259e-02],\n",
      "        [-2.0100e-01, -2.2425e-01, -4.6327e-01,  ..., -8.9174e-01,\n",
      "          1.2615e+00, -1.1217e+00]])\n",
      "[tensor(0.6995), tensor(0.7715), tensor(0.6956), tensor(0.7078), tensor(0.6727), tensor(0.7677), tensor(0.6087), tensor(0.8411), tensor(0.6926), tensor(0.7613), tensor(0.6736), tensor(0.6567), tensor(0.7819), tensor(0.7372), tensor(0.8181), tensor(0.8384), tensor(0.6129), tensor(0.7030), tensor(0.8258), tensor(0.5918), tensor(0.7964), tensor(0.6725), tensor(0.7866), tensor(0.6531), tensor(0.8535), tensor(0.7243), tensor(0.7342), tensor(0.7652), tensor(0.6910), tensor(0.7630), tensor(0.6750), tensor(0.6235), tensor(0.6592), tensor(0.7998), tensor(0.7464), tensor(0.7636), tensor(0.7566), tensor(0.6457), tensor(0.7702), tensor(0.6708), tensor(0.6576), tensor(0.7156), tensor(0.6303), tensor(0.6113), tensor(0.7331), tensor(0.8240), tensor(0.6284), tensor(0.6464), tensor(0.8085), tensor(0.7505), tensor(0.6544), tensor(0.8338), tensor(0.7073), tensor(0.8125), tensor(0.6914), tensor(0.7069), tensor(0.7014), tensor(0.7595), tensor(0.7109), tensor(0.7345), tensor(0.6712), tensor(0.7650), tensor(0.8082), tensor(0.7564), tensor(0.7028), tensor(0.6741), tensor(0.5844), tensor(0.7358), tensor(0.5069), tensor(0.7799), tensor(0.6749), tensor(0.8204), tensor(0.7017), tensor(0.8015), tensor(0.7444), tensor(0.7399), tensor(0.7900), tensor(0.7120), tensor(0.7924), tensor(0.6767), tensor(0.7615), tensor(0.7207), tensor(0.7538), tensor(0.6843), tensor(0.6152), tensor(0.7425), tensor(0.7323), tensor(0.6510), tensor(0.7429), tensor(0.6467), tensor(0.7665), tensor(0.7156), tensor(0.6664), tensor(0.6070), tensor(0.7267), tensor(0.7965), tensor(0.6945), tensor(0.6241), tensor(0.6665), tensor(0.7682)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "bce_loss = torch.nn.BCELoss()\n",
    " \n",
    "crit = torch.nn.SmoothL1Loss()\n",
    "pre = torch.Tensor([0.5, 1.6, 1])\n",
    "target =  torch.Tensor([0.5, 0.8, 1])\n",
    "x = torch.randn(100, 100)\n",
    "y = torch.randn(100, 100)\n",
    "target2 =  torch.Tensor([0.5, 0.9, 1])\n",
    "target3 =  torch.Tensor([0.5, 0.8, 1.1])\n",
    "print(pre.size())\n",
    "print(target.size())\n",
    "print('SmoothL1Loss',crit(pre, target))\n",
    "print('SmoothL1Loss2',torch.nn.functional.smooth_l1_loss(pre,target2)) #target放在右边，pre只能在0-1之间\n",
    "print('SmoothL1Loss2',torch.nn.functional.smooth_l1_loss(pre,target3)) #target放在右边，pre只能在0-1之间\n",
    "# print('bce2',bce_loss(pre, target))\n",
    "print(x)\n",
    "print(y)\n",
    "loss = [crit(a,b) for a,b in zip(x,y)]\n",
    "print(loss)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)  # 设置图像显示大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # 设置差值方式\n",
    "plt.rcParams['image.cmap'] = 'gray'  # 设置灰度空间\n",
    "plt.plot(np.squeeze(loss))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate = 0.01\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02a3cc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'tentst']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"a b c tentst\"\n",
    "a.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d8aa7",
   "metadata": {},
   "source": [
    "# 4.测试获取实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef619ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_entity(ix_seq, sentence):\n",
    "    ix_to_tag = {  # 标签词典 {标注——索引}\n",
    "        0: \"NAME\",  1: \"TICKER\",  2: \"NOTIONAL\",\n",
    "        # 3: \"I-NAM\",  4: \"I-TIC\",  5: \"I-NOT\",\n",
    "        # 6: \"0\", 7: \"<START>\", 8: \"<START>\"\n",
    "    }\n",
    "    # tag_seq = [ix_to_tag[ix] for ix in ix_seq]\n",
    "    entities = defaultdict(str)\n",
    "    tokens = sentence.split()\n",
    "    i = 0\n",
    "    while i < len(ix_seq):\n",
    "        ix = ix_seq[i]\n",
    "        if ix in ix_to_tag:\n",
    "            j = i+1\n",
    "            while j < len(ix_seq) and ix_seq[j] == ix+3:\n",
    "                j += 1\n",
    "            entities[ix_to_tag[ix]] = ' '.join(tokens[i: j])\n",
    "            i = j-1\n",
    "        i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4fee5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "读取训练数据: 100%|█████████████████████████████████████████████████████████████| 2400/2400 [00:00<00:00, 14735.53it/s]\n",
      "读取测试数据: 100%|███████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 15409.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'NAME': 'Amy Nelson', 'NOTIONAL': '506.807b', 'TICKER': 'EXROF'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"Hi Douglas Bell K , Can I Call 8.294t UUUU\"\n",
    "# tag_to_ix = {\"B-NAM\": 0, \"B-TIC\": 1, \"B-NOT\": 2, \n",
    "#              \"I-NAM\": 3, \"I-TIC\": 4, \"I-NOT\": 5,\n",
    "#              \"O\": 6}  # 标签词典 {标注——索引}\n",
    "# tag_seq = ['O', 'O', 'O', 'O', 'O', 'O', 'B-NOT', 'I-NOT', 'I-NOT', 'I-NOT', 'I-NOT', 'I-NOT', 'I-NOT', 'O', 'B-TIC', 'I-TIC', 'I-TIC', 'I-TIC', 'I-TIC']\n",
    "# # ix_seq = [tag_to_ix[tag] for tag in tag_seq]\n",
    "# ix_seq = [6, 0, 3, 3, 6, 6, 6, 6, 2, 1]\n",
    "# get_entity(ix_seq, sentence)\n",
    "import torch\n",
    "from bilstm_crf import prepare_sequence\n",
    "from read_data import read\n",
    "def nlp(sentence):\n",
    "    pre_model = torch.load('pre_model.pth')  # 直接加载模型\n",
    "    data_set, word_to_ix = read('./data/data.json')\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        model_in = prepare_sequence(tokens, word_to_ix)\n",
    "        model_out = pre_model(model_in)\n",
    "        nlp_result = get_entity(model_out[1], sentence)\n",
    "    # nlp_result = {\n",
    "    #     \"NAME\": \"\",\n",
    "    #     \"TICKER\": \"\",\n",
    "    #     \"NOTIONAL\": \"\"\n",
    "    # }\n",
    "    return nlp_result\n",
    "\n",
    "nlp('Hello Amy Nelson , 506.807b EXROF sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e9e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46b6019",
   "metadata": {},
   "source": [
    "# 5.模型训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c0f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27be3031",
   "metadata": {},
   "source": [
    "# 6.尝试batch（失败）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df00ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n",
      "tensor([12., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "x = torch.linspace(1,12,12)\n",
    "y = torch.linspace(12,1,12)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = TensorDataset(x,y)\n",
    "loader = DataLoader(dataset = torch_dataset, batch_size = BATCH_SIZE, \n",
    "                         shuffle = True, num_workers = 2)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        print(f'Epoch:{epoch} | num:{i} || batch_x:{batch_x} || batch_y:{batch_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(1)    # 人工设定随机种子以保证相同的初始化参数，实现模型的可复现性。\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "\n",
    "def argmax(vec):  # 给定输入二维序列，取每行（第一维度）的最大值，返回对应索引。\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix, tags, tag_to_ix, max_length):    # 利用to_ix这个word2id字典，将序列seq中的词转化为数字表示，包装为torch.long后返回\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tidxs = [tag_to_ix[tag] for tag in tags]\n",
    "    length = len(idx)\n",
    "    for i in range(max_length-length:\n",
    "        idxs.append(0)\n",
    "        tidxs.append(6)\n",
    "#     return torch.tensor(idxs, dtype=torch.long)\n",
    "    return idxs, tidxs\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):                # 函数目的相当于log∑exi 首先取序列中最大值，输入序列是一个二维序列(shape[1,tags_size])。下面的计算先将每个值减去最大值，再取log_sum_exp，最后加上最大值。\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "\n",
    "# def compute_loss(self, pre, target):\n",
    "#     loss_func = torch.nn.SmoothL1Loss()\n",
    "#     loss = [loss_func(x, y) for x, y in zip(pre, target)]\n",
    "#     plt.rcParams['figure.figsize'] = (5.0, 4.0)  # 设置图像显示大小\n",
    "#     plt.rcParams['image.interpolation'] = 'nearest'  # 设置差值方式\n",
    "#     plt.rcParams['image.cmap'] = 'gray'  # 设置灰度空间\n",
    "#     plt.plot(np.squeeze(loss))\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate = 0.01\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim  # 词嵌入维度，即输入维度\n",
    "        self.hidden_dim = hidden_dim  # 隐层维度\n",
    "        self.vocab_size = vocab_size  # 训练集词典大小\n",
    "        self.tag_to_ix = tag_to_ix  # 标签索引表\n",
    "        self.tagset_size = len(tag_to_ix)  # 标注 类型数\n",
    "        print(f'tagset_size={self.tagset_size}')\n",
    "        self.word_embeds = nn.Embedding(vocab_size,\n",
    "                                        embedding_dim)  # （词嵌入的个数，嵌入维度）\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2,\n",
    "                            # （输入节点数，隐层节点数，隐层层数，是否双向）\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)  # hidden_size除以2是为了使BiLSTM\n",
    "        # 的输出维度依然是hidden_size,而不用乘以2\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        # （输入x的维度，输出y的维度），将LSTM的输出线性映射到标签空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(  # 转移矩阵，标注j转移到标注i的概率，后期要学习更新\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000  # 不会有标注转移到开始标注\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000  # 结束标注不会转移到其他标注\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):  # 初始化隐层（两层，3维）\n",
    "        # (num_layer * num_direction, batch_size)\n",
    "        # (隐层层数2 * 方向数1， 批大小1， 每层节点数)\n",
    "        return (torch.randn(2, 1, self.hidden_dim//2),\n",
    "                torch.randn(2, 1, self.hidden_dim//2))\n",
    "\n",
    "    def _forward_alg(self, feats):  # 得到所有路径的分数/概率\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size),\n",
    "                                 -10000.)  # P，(1, m)维，初始化为-10000\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas  # 前向状态，记录当前t之前的所有路径的分数\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:  # 动态规划思想，具体见onenote上的笔记\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1,\n",
    "                                                               self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var+trans_score+emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var+self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha  # 返回的是所有路径的分数\n",
    "\n",
    "    def _get_lstm_features(self, sentence):  # 通过BiLSTM层，输出得到发射分数\n",
    "        self.hidden = self.init_hidden()\n",
    "        # 对输入语句 词嵌入化\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        # 词嵌入通过lstm网络输出,lstm传入参数之后会自动调用其forward方法\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        # 将输出转为2维（原本是3维，但是batch_size=1，可以去掉这一维）\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)  # 将输出映射到标签空间，得到单词-分数表\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):  # 计算给定路径的分数\n",
    "        # feats : LSTM的所有输出，发射分数矩阵\n",
    "        # tags : golden路径的标注序列\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat(\n",
    "            [torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long),\n",
    "                tags])  # 在标注序列最前加上开始标注\n",
    "        for i, feat in enumerate(feats):  # 计算给定序列的分数，Σ发散分数+Σ转移分数\n",
    "            score = score+self.transitions[tags[i+1], tags[i]]+feat[tags[i+1]]\n",
    "        score = score+self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        # 初始化forward_var,并且 开始标注 的分数为0,确保一定是从START_TAG开始的,\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        # forward_var记录每个标签的前向状态得分，即w{i-1}被打作每个标签的对应得分值\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        # feats是LSTM的输出，每一个feat都是一个词w{i}，feat[tag]就是这个词tag标注的分数\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step记录当前词w{i}对应每个标签的最优转移结点\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step 记录当前词各个标签w{i, j}对应的最高得分\n",
    "            # 动态规划：w{i，j}=max{forwar_var + transitions[j]}，词存于bptrs_t中，分数存于viterbivars_t中\n",
    "\n",
    "            for next_tag in range(self.tagset_size):  # 对当前词w{i}的每个标签 运算\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var+self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t)+feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)  # 记忆，方便回溯\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var+self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)  # 结束标记前的一个词的最高前向状态得分就是最优序列尾\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):  # 回溯\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):  # CRF的损失函数：-gold分数-logsumexp(所有序列)\n",
    "        feats = self._get_lstm_features(\n",
    "            sentence)  # 通过BiLSTM层，获得每个 {词-标签}对 的发射分数\n",
    "        forward_score = self._forward_alg(feats)  # 根据发射分数计算所有路径的分数\n",
    "        gold_score = self._score_sentence(feats, tags)  # 传入标注序列真实值，计算语句的真实分数gold_score\n",
    "        return forward_score-gold_score  # 返回误差值\n",
    "\n",
    "    def forward(self, sentence):  # 重载前向传播函数，对象传入参数后就会自动调用该函数\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)  # 通过LSTM层得到输出\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)  # 通过CFR层得到最优路径及其分数\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# from bilstm_crf import *\n",
    "from read_data import read\n",
    "\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5  # 词嵌入维度\n",
    "HIDDEN_DIM = 4  # 隐层层数\n",
    "tag_to_ix = {  # 标签词典 {标注——索引}\n",
    "    \"B-NAM\": 0, \"B-TIC\": 1, \"B-NOT\": 2,\n",
    "    \"I-NAM\": 3, \"I-TIC\": 4, \"I-NOT\": 5,\n",
    "    \"O\": 6, START_TAG: 7, STOP_TAG: 8\n",
    "}\n",
    "data_set, word_to_ix = read('./data/data.json')  # 数据集，词典 {词——索引}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d599c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(pre, target):\n",
    "    loss_func = torch.nn.SmoothL1Loss()\n",
    "    loss = [loss_func(x, y) for x, y in zip(pre, target)]\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 4.0)  # 设置图像显示大小\n",
    "    plt.rcParams['image.interpolation'] = 'nearest'  # 设置差值方式\n",
    "    plt.rcParams['image.cmap'] = 'gray'  # 设置灰度空间\n",
    "    plt.plot(np.squeeze(loss))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate = 0.01\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_batch(data_input, data_label, batch_size):\n",
    "    print(data_input[0])\n",
    "    print(data_label[0])\n",
    "    print(torch.Tensor(data_input).size(0))\n",
    "    print(len(data_label))\n",
    "#     data_input = torch.Tensor(data_input)\n",
    "#     data_label = torch.Tensor(data_label)\n",
    "#     print(len(data_input))\n",
    "#     print(len(data_label))\n",
    "    data = TensorDataset(torch.tensor(data_input, dtype=torch.long), torch.tensor(data_label, dtype=torch.long))\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False) #shuffle是是否打乱数据集，可自行设置\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def train():\n",
    "    # data_set, word_to_ix = read('./data/data.json')  # 数据集，词典 {词——索引}\n",
    "    training_data = data_set['training']\n",
    "\n",
    "    model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    data_in, data_label = [], []\n",
    "    max_length = len(max(training_data, key = lambda x: len(x[0]))[0])\n",
    "    for tokens, tags in training_data:\n",
    "        token_ix, tag_ix = prepare_sequence(tokens, word_to_ix, tags, tag_to_ix, max_length)\n",
    "        data_in.append(token_ix)\n",
    "#         data_label.append(torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long))\n",
    "        data_label.append(tag_ix)\n",
    "    training_batch = add_batch(data_in, data_label, 100)\n",
    "    # Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "    for epoch in trange(300, desc='模型训练进度'):\n",
    "        bar = tqdm(enumerate(training_batch), leave=False)\n",
    "        for step, (sentence, tags) in bar:\n",
    "            bar.set_description(f'epoch【{epoch}】')\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "#             print('=======')\n",
    "#             print(sentence.size(0))\n",
    "#             print(tags)\n",
    "            # Step 2. Get our inputs ready for the network, that is,\n",
    "            # turn them into Tensors of word indices.\n",
    "#             sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "#             targets = torch.tensor([tag_to_ix[t] for t in tags],\n",
    "#                                    dtype=torch.long)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            loss = model.neg_log_likelihood(sentence, tags)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0: # https://blog.csdn.net/u013250861/article/details/124657801\n",
    "                state = {\n",
    "                    'epoch' : epoch + 1,                  # 保存当前的迭代次数\n",
    "                    'state_dict' : model.state_dict(),    # 保存模型参数\n",
    "                    'optimizer' : optimizer.state_dict(), # 保存优化器参数\n",
    "                }\n",
    "                torch.save(state, f'./checkpoint2/checkpoint{epoch}.pth.tar')\n",
    "    print('training over!')\n",
    "    torch.save(model, 'pre_model.pth')\n",
    "    torch.save(model.state_dict(), 'model_params.pth')\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf5c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f20b082",
   "metadata": {},
   "source": [
    "# 7.词典转文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bef91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set = {'NAME':set(), 'TICKER':set(), 'NOTIONAL':set()}\n",
    "O_set = set()\n",
    "tag_to_ix = {  # 标签词典 {标注——索引}\n",
    "    \"B-NAM\": 0, \"B-TIC\": 1, \"B-NOT\": 2,\n",
    "    \"I-NAM\": 3, \"I-TIC\": 4, \"I-NOT\": 5,\n",
    "    \"O\": 6, START_TAG: 7, STOP_TAG: 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40892c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(ix_seq, sentence):\n",
    "    ix_to_tag = {0: \"NAME\", 1: \"TICKER\", 2: \"NOTIONAL\", }\n",
    "    # tag_seq = [ix_to_tag[ix] for ix in ix_seq]\n",
    "    entities = defaultdict(str)\n",
    "    tokens = sentence.split()\n",
    "    i = 0\n",
    "    while i < len(ix_seq):\n",
    "        ix = ix_seq[i]\n",
    "        if ix in ix_to_tag:\n",
    "            j = i+1\n",
    "            while j < len(ix_seq) and ix_seq[j] == ix+3:\n",
    "                j += 1\n",
    "            entities[ix_to_tag[ix]] = ' '.join(tokens[i: j])\n",
    "            entity_set[ix_to_tag[ix]].add(' '.join(tokens[i: j]))\n",
    "            i = j-1\n",
    "        else:\n",
    "            O_set.add(tokens[i])\n",
    "        i += 1\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edfaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def get_tags(label, length):\n",
    "#     tags = ['O'] * length\n",
    "#     for entity_type, [start, end] in label:\n",
    "#         entity_type = entity_type[0:3]\n",
    "#         tags[start] = 'B-' + entity_type\n",
    "#         for i in range(start+1, end):\n",
    "#             tags[i] = 'I-' + entity_type\n",
    "#     return tags\n",
    "\n",
    "\n",
    "def get_tags(label, tokens):\n",
    "    length = len(tokens)\n",
    "    tags = ['O'] * length\n",
    "    index = []*length\n",
    "    last = 0\n",
    "    for i in range(length):\n",
    "        index.append(last+len(tokens[i]))\n",
    "        last = index[-1]+1\n",
    "#     print(index)\n",
    "    label.sort(key = lambda x: x[1][0])\n",
    "    help_map = {0:'B-', 1:'I-'}\n",
    "#     print(label)\n",
    "    label2 = [(l, x[0:3]) for x, y in label for l in y]\n",
    "#     print(label2)\n",
    "    i, j = 0, 0\n",
    "    while i < length and j < len(label):    # 双指针\n",
    "        if label[j][1][0] < index[i] <= label[j][1][1]:\n",
    "            tags[i] = label[j][0][0:3]\n",
    "        if index[i] > label[j][1][1]:\n",
    "            j += 1\n",
    "            i -= 1\n",
    "        i += 1\n",
    "    for i in range(length-1, -1, -1):\n",
    "        if tags[i] != 'O':\n",
    "            if i == 0 or tags[i-1] != tags[i]:\n",
    "                tags[i] = f'B-{tags[i]}'\n",
    "            else:\n",
    "                tags[i] = f'I-{tags[i]}'\n",
    "    return tags\n",
    "\n",
    "\n",
    "def read(file):\n",
    "    df = pd.read_json(file)\n",
    "    data_size = len(df)\n",
    "    data_set = []\n",
    "    for i in tqdm(range(0, data_size), desc='读取数据', position=0):\n",
    "        text, label = df.loc[i]\n",
    "        with open('./data/sentence.txt', 'w+') as f:\n",
    "            f.writelines(f'{text}\\n')  #文件的写操作\n",
    "        tokens = text.split()\n",
    "        tags = get_tags(label, tokens)\n",
    "        result = get_entity([tag_to_ix[tag] for tag in tags], text)\n",
    "        data_set.append((tokens, tags))\n",
    "        \n",
    "    df2 = pd.DataFrame(data_set)\n",
    "    return data_set, df2\n",
    "\n",
    "_, dd = read('./data/data.json')\n",
    "\n",
    "# with open('./data/NAME.txt', 'w') as f:\n",
    "#     for s in entity_set['NAME']:\n",
    "#         f.writelines(f'{s}\\n')  #文件的写操作\n",
    "# with open('./data/TICKER.txt', 'w') as f:\n",
    "#     for s in entity_set['TICKER']:\n",
    "#         f.writelines(f'{s}\\n')  #文件的写操作\n",
    "# with open('./data/NOTIONAL.txt', 'w') as f:\n",
    "#     for s in entity_set['NOTIONAL']:\n",
    "#         f.writelines(f'{s}\\n')  #文件的写操作\n",
    "# with open('./data/O.txt', 'w') as f:\n",
    "#     for s in O_set:\n",
    "#         f.writelines(f'{s}\\n')  #文件的写操作\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5937fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
